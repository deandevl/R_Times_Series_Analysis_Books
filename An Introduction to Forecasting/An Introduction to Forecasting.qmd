---
title: "An Introduction to Forecasting"
author: "Rick Dean"
format:
  html:
    toc: false
    toc-depth: 4
    toc-location: "right"
    number-sections: true
    number-offset: 0
    self-contained: true
    smooth-scroll: true
    code-fold: true
    code-block-bg: "#f1f3f5"
    code-block-border-left: "#31BAE9"
    code-overflow: wrap
    tbl-cap-location: "bottom"
    fig-width: 5
    fig-height: 5
    fig-align: "center"
    fig-cap-location: "bottom"
    minimal: false
    css: ../style.css
    link-external-newwindow: true
    abstract-title: "Abstract"
    abstract: "The following notes/scripts/plots are inspired by an excellent article [An Introduction to Forecasting](https://www.datascienceblog.net/post/machine-learning/forecasting-an-introduction/) by Matthias DÃ¶ring"
---

```{r}
#| warning: false
#| message: false

library(data.table)
library(anytime)
library(tseries)
library(zoo)
library(astsa)
library(ggplot2)
library(grid)
library(gtable)
library(RtsaPkg)
library(RplotterPkg)
```

# Important Concepts

##The backshift operator

Given the time series $y = \lbrace y_1, y_2,...\rbrace$, the backshift operator (also called the lag operator) is defined as:

$$B_{y_t} = y_{t-1}, \forall t > 1$$.

One application of the backshift operator yields the previous measurement in the time series. Raising the backshift operator to a power *k* \> 0 performs multiple shifts at once:

$$B^ky_t = y_{t-k}$$ $$B^{-k}y_t = y_{t+k}$$

For example $B^2y_t$ yields the measurement that was observed two time periods earlier. Instead of $B$, $L$ is used equivalently to indicate the lag operator.

::: topic
Lagged differences with the backshift operator
:::

The backshift operator can be used to calculate lagged differences for a time series of values via $y_i - B^k(y_i), \forall i \in k + 1,...,t$ where *k* indicates the lag of the differences. For *k* = 1 we obtain ordinary pairwise differences.

Using the `base::diff()` function to calculate lagged differences. The second arg sets the desired lag. Default is 1.

```{r}
y <- c(1,3,5,10,20)
(By_1 <- diff(y))
(By_3 <- diff(y,3))
```

## The autocorrelation function

Defines the correlation of a variable $y_t$ to previous measurements $y_{t-1}, \dotsb y_1$ of the same variable (hence the name autocorrelation).

The autocorrelation for lag *k* is defined as: $$\varphi_k := Corr(y_t, y_{t-k}) k = 0,1,2,\dotsb$$

A function that constructs two vectors, $y_t$ and $y_{t-k}$ according to the *lag* argument:

```{r}
autocor <- function(x,lag=1){       # [1, 3,  5, 10, 20]
  x.left <- x[1:(length(x) - lag)]  # [3, 5, 10, 20,]
  x.right <- x[(1+lag):(length(x))] # [1, 3,  5, 10,]
  val <- stats::cor(x.left, x.right)
  return(val)
}
(y_cor_1 <- autocor(y))
(y_cor_2 <- autocor(y,2))
```

## Partial autocorrelations

Note that the autocorrelation (ACF) function does not control for the other lags. The partial autocorrelation (pACF) does regress the values of the time series at all shorter lags.

The partial autocorrelation (pACF) at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags. Given a time series $z_t$, the pACF is the autocorrelation between $z_t$ and $z_{t+k}$ with the linear dependence of $z_t$ on $z_{t+1}$ and $z_{t+k-1}$ removed.

The partial autocorrelation of an AR(k) process is zero at lags k + 1 and greater. To help determine the order of an observed process, one looks at the point on the plot where the partial autocorrelation for all higher lags are essentially zero.

An approximate test that a given partial correlation is zero (at a 5% significance level) is given by comparing the sample partial autocorrelation against the critical region with the upper and lower limits given by +-1.96/sqrt(N), where N is the record length of the time series. This approximation relies on the assumption that the record length is at least moderately large (N\>30) and that the underlying process has a finite second moment.

```{r}
#| warning: false
#| fig-width: 10
#| fig-cap: ACF and pACF of a simple 11 point series

y_df <- data.frame(
  time = 1:5,
  series = c(1,3,5,10,20)
)
y_acf <- RtsaPkg::graph_acf(
  df = y_df,
  time_col = "time",
  value_col = "series",
  max_lag = 4,
  bold_y = 0.0,
  ac_y_limits = c(-0.4, 1),
  ac_y_major_breaks = seq(-0.4,1,0.2),
  pac_y_limits = c(-0.4,1),
  pac_y_major_breaks = seq(-0.4,1,0.2),
  show_minor_grids = FALSE,
  line_width = 1.2,
  line_color = "brown"
)
```

## Decomposing time-series data

Asking whether the time series data are additive or multiplicative. The main difference between additive and multiplicative time series is the following:

1.  Additive: amplitudes of seasonal effects are similar in each period
2.  Multiplicative: seasonal trend changes with the progression of the time series

::: topic
Multiplicative time series `datasets::AirPassengers` data set.
:::

A multiplicative example from time series object (of class 'ts') `datasets::AirPassengers`:

Show the time series info for `datasets::AirPassengers`.

```{r}
#| tbl-cap: Time series information for datasets::AirPassengers

RplotterPkg::create_table(
  x = RtsaPkg::get_series_info(series = datasets::AirPassengers)$stats,
  container_width_px = 450
)
```

`datasets::AirPassengers` is a monthly time series (class = "ts") that starts from January, 1949 to December, 1960 (144 months).

```{r}
#| fig-width: 8
#| fig-cap: Box & Jenkins Airline Data

airpass_dt <- RtsaPkg::tsObj_to_dt(datasets::AirPassengers)
RplotterPkg::create_scatter_plot(
    df = airpass_dt,
    aes_x = "time",
    aes_y = "value",
    x_title = "Month",
    y_title = "Air Passengers",
    connect = TRUE,
    rot_y_tic_label = TRUE,
    x_major_breaks = seq(as.Date("1949-01-01"), as.Date("1960-12-01"), by = "1 year"),
    x_date_labels = "%Y",
    y_major_breaks = seq(100,600,50),
    show_minor_grids = FALSE
)
```

The amplitude of the seasonal trend is increasing through the years. To adjust for this effect, we have to take the logarithm of the measurements. The logarithm turns the multiplicative model into an additive: $log(S_iT_t\epsilon_t) = log(S_t) + log(T_t) + log(\epsilon_t)$.

::: topic
Plotting the measurements on a log10 scale
:::

```{r}
#| fig-width: 8
#| fig-cap: Box & Jenkins Airline Data, on a log10 scale

RplotterPkg::create_scatter_plot(
    df = airpass_dt,
    aes_x = "time",
    aes_y = "value",
    x_title = "Month",
    y_title = "log(Air Passengers)",
    y_log10 = TRUE,
    connect = TRUE,
    rot_y_tic_label = TRUE,
    x_major_breaks = seq(as.Date("1949-01-01"), as.Date("1960-12-01"), by = "1 year"),
    x_date_labels = "%Y",
    y_major_breaks = seq(100,600,50),
    show_minor_grids = FALSE
)
```

The logarithm scale has equalized the amplitude of the seasonal component along time. Note that the overall trend has not changed.

::: topic
Using multiplicative decomposition
:::

Plot `datasets::AirPassengers` decomposed into its observed, seasonal, trend, and residual components.

```{r}
#| fig-width: 8
#| fig-height: 6
#| fig-cap: Decompose Box & Jenkins Airline Data

airpass_decompose <- RtsaPkg::graph_decompose(
  series_ts = datasets::AirPassengers,
  type_comp = "multiplicative",
  x_title = "Year"
)
```

There appears to be an increasing trend and the seasonal component is consistent.

::: topic
Using additive decomposition
:::

`datasets::EuStockMarkets` is a multiple time series object (mts) so select the German series.

```{r}
german_ts <- datasets::EuStockMarkets[,1]
RplotterPkg::create_table(
  x = RtsaPkg::get_series_info(series = german_ts)$stats,
  container_width_px = 400
)
```

`german_ts` is a time series (class = ts) that has 260 data points per year from the middle of 1991 to 3/4's of 1998 (1860 total sampled points).

Decompose `german_ts`. Note that "additive" is the default type decomposition.

```{r}
#| fig-width: 8
#| fig-height: 6
#| fig-cap: Decompose German Stock Market Daily Closing Prices

german_decompose <- RtsaPkg::graph_decompose(
  series_ts = german_ts,
  x_title = "Year"
)
```

Shows an overall increasing trend with a seasonal peak in the end of summer. The random noise hovers around zero except toward the end of the series.

## Stationary vs. non-stationary processes

A process is stationary if its mean and variance are not shifting along the time line. Both `datasets::AirPassengers` and `datasets::EuStockMarkets` are non-stationary because they both have increasing trends throughout the time line.

::: topic
A stationary process
:::

View the information of `nino3.4` series.

```{r}
data(nino)
RplotterPkg::create_table(
  x = RtsaPkg::get_series_info(series = nino3.4)$stats,
  container_width_px = 400
)
```

`nino3.4` is a monthly time series of class 'ts' that starts January 1950 and ends October 1999 for 598 months.

```{r}
#| code-fold: true
#| fig-width: 8 
#| fig-cap: Nino Region 3.4 SST Monthly Temperatures (deg C)

nino3.4_dt <- RtsaPkg::tsObj_to_dt(series = nino3.4)
RplotterPkg::create_scatter_plot(
    df = nino3.4_dt,
    aes_x = "time",
    aes_y = "value",
    x_title = "Year",
    y_title = "Temperature",
    rot_y_tic_label = TRUE,
    x_major_breaks = seq(as.Date("1950-01-01"), as.Date("2000-12-01"), by = "4 year"),
    x_date_labels = "%Y",
    show_pts = FALSE,
    connect = TRUE
)
```

Very little trend across the time line.

## The ARMA model

ARMA stands for autoregressive moving average. ARMA models are only appropriate for stationary processes and have two parameters:

-   $p$: the order of the autoregressive (AR) model
-   $q$ the order of the moving average (MA) model

The ARMA model can be specified as: $$ \hat{y}_t = c + \epsilon_t + \sum_{i=1}^p\phi_iy_{t-i} - \sum_{j=1}^q\theta_j\epsilon_{t-j} $$ with the following variables:

-   $c$: the intercept of the model (e.g. the mean)
-   $\epsilon_t$: random error (white noise, residual) associated with measurement *t* with $\epsilon_t \sim N(0,\sigma)$.
-   $\phi \in \mathbb{R}^p$: a vector of coefficients for the AR terms. In R, these parameters are called *AR1*, *AR2*, etc.
-   $y_t$: outcome measured at time *t*.
-   $\theta \in \mathbb{R}^q$: a vector of coefficients for the MA terms. In R, these parameters are called *MA1*, *MA2*, etc.
-   $\epsilon_t$: noise associated with measurement *t*.

::: topic
Formulating the ARMA model using the backshift operator
:::

Using the backshift operator, we can formulate the ARMA model in the following way: $$(1 - \sum_{i=1}^p\phi_iB^i)y_t = (1 - \sum_{j=1}^q\theta_jB^j)\epsilon_j$$

By defining $\phi_p(B)$ = $1 - \sum_{i=1}^p\phi_iB^i$ and $\theta_q(B)$ = $1 - \sum_{j=1}^q\theta_jB^j$, the ARMA model simplifies to: $$\phi_p(B)y_t = \theta_q(B)\epsilon_t $$.

## The ARIMA model

ARIMA stands for autoregressive integrated moving average and is a generalization of the ARMA model. In contrast to ARMA models ARIMA models are capable of dealing with non-stationary data, that is, time-series where the mean and variance changes over time. This feature is indicated by the (integrated) of ARIMA: an initial differencing step can eliminate the non-stationarity. For this purpose ARIMA require an additional parameter, d. Taken together an ARIMA model has the following three parameters:

-   $p$: the order of the autoregressive (AR) model
-   $d$: the degree of differencing
-   $q$: the order of the moving average (MA) model

In the ARIMA model, outcomes are transformed to differences by replacing $y_t$ with differences of the form: $$(1 - B)^dy_t$$ The model is then specified by $$\phi_p(B)(1 - B)^dy_t = \theta_q(B)\epsilon_t$$

For $d = 0$ the model simplifies to the ARMA model since $(1 - B)^0y_t = y_t$. For other choices of $d$ we obtain backshift ploynomials, for example:

$$(1 - B)^1y_t = y_t - y_{t-1}$$

$$(1 - B)^2y_t = (1 -2B + B^2)y_t = y_t - 2y_{t-1} + y_{t-2}$$

In the following let us consider the interpretation of the three parameters of ARIMA models.

### ARIMA model and the impact of $p$

The parameter $p\in \mathbb{N}_0$ specifies the order of the autoregressive model. The term *order* refers to the number of lagged differences that the model considers. For simplicity let us assume that $d = 0$ (no differencing). Then an AR model of order 1 considers only the most recent measurements, that is, $By_t = y_{t-1}$ via the parameter $\phi_1$. An AR model of order 2, on the other hand would consider the last two points in time, that is, measurements $y_{t-1}$ as well as $y_{t-2}$ through $\phi_1$ and $\phi_2$, respectively.

The number of autoregressive terms indicates the extent to which previous measurements influence the current outcome. For example, ARIMA(1,0,0), which has $p = 1$, $d = 0$ and $q = 0$, has an autoregressive term of order 1, which means that the outcome is influenced only by the most recent previous measurements. In this case the model simplifies to $$\hat{y}_t = \mu\epsilon_t + \phi_1y_{t-1}$$

Question: How do we get this simplication with the product of $\mu\epsilon_t$?

::: topic
Simulate ARIMA(1,0,0) autoregression series.
:::

We can simulate autoregressive processes using the `stats::arima.sim()` function. Via the function the model can be specified by providing the coefficients for the MA and AR terms to be used. In the following we will plot the autocorrelation, because it is best suited for finding the impact of autoregression.

```{r}
set.seed(5)
ar_1 <- stats::arima.sim(list(ar = 0.75), n = 1000)
ar_1_data <- RtsaPkg::get_series_data(series = ar_1)
ar_1_ts <- stats::ts(data = ar_1_data, start = c(1900,1), frequency = 12)
```

For plotting purposes we've set the time from January, 1900 to April, 1983 for 1000 monthly points.

```{r}
#| fig-width: 8 
#| fig-cap: Autocorrelations of Simulated ARIMA(1,0,0) Series

ar_1_dt <- RtsaPkg::tsObj_to_dt(series = ar_1_ts)
ar_1_ar <- RtsaPkg::graph_acf(
  df = ar_1_dt,
  time_col = "time",
  value_col = "value",
  max_lag = 30,
  confid_level = 1.96,
  ac_x_limits = c(1,30),
  ac_x_major_breaks = seq(1,30,1),
  ac_y_limits = c(-0.2, .8),
  pac_y_limits = c(-0.2, .8),
  bold_y = 0.0,
  show_minor_grids = FALSE,
  line_color = "brown"
)
```

Shows a very high correlation at lag 1.

::: topic
Simulate a second order autoregressive process ARIMA(2,0,0)
:::

```{r}
ar_2 <- stats::arima.sim(list(ar = c(0.65, 0.3)), n = 1000)
ar_2_data <- RtsaPkg::get_series_data(series = ar_2)
ar_2_ts <- stats::ts(data = ar_2_data, start = c(1900,1), frequency = 12)
```

```{r}
#| fig-width: 8
#| fig-cap: Autocorrelations of Simulated ARIMA(2,0,0) Series

ar_2_dt <- RtsaPkg::tsObj_to_dt(series = ar_2_ts)
ar_2_ar <- RtsaPkg::graph_acf(
  df = ar_2_dt,
  time_col = "time",
  value_col = "value",
  max_lag = 30,
  confid_level = 1.96,
  ac_x_limits = c(1,30),
  ac_x_major_breaks = seq(1,30,1),
  ac_y_limits = c(-0.2, 1.0),
  pac_y_limits = c(-0.2, 1.0),
  bold_y = 0.0,
  show_minor_grids = FALSE,
  line_color = "brown"
)
```

Very high correlations at lags 1 and 2.

The order of the AR term can be selected according to the largest lag at which the pACF was significant.

### ARIMA model and $d$ (degree of differencing)

The parameter $d \in \mathbb{N}_0$ specifies the degree of differencing in the model term $(1 - B)^dy_t$ In practice $d$ should be chosen such that we obtain a stationary process. An ARIMA(0,1,0) model simplifies to the random walk model $$\hat{y}_t = \mu + \epsilon + y_{t-1}$$ The model is random because for every point in time $t$ the mean is simply adjusted by $y_{t-1}$, which leads to random changes of $y_t$ over time.

Plot the first order differences using `RtsaPkg::graph_dif()`:

```{r}
#| code-fold: true
#| fig-width: 8 
#| fig-cap: AirPassengers Monthly Totals First Order Differences

AirPassengers_dt <-  RtsaPkg::tsObj_to_dt(series = datasets::AirPassengers)
AirPassengers_dif <- RtsaPkg::graph_dif(
  df = AirPassengers_dt,
  time_col = "time",
  value_col = "value",
  title = "",
  x_title = "Year",
  y_title = "Passengers"
)
```

The *AirPassengers* observations are seasonal with increasing trend while by taking differences of the series the lagged differences are stationary.

### ARIMA model and the impact of $q$ (the moving average)

The moving average model is specified via $q\in\mathbb{N}_0$. The MA term models the past error, $\epsilon_t$ using coefficients $\theta$. An ARIMA(0,0,1) model simplifies to: $$\hat{y}_t = \phi + \epsilon_t + \theta_1\epsilon_{t-1}$$ in which the current estimate depends on the residual of the previous measurement.

Show the impact of the moving average by simulating and plotting a ARIMA(0,0,1) process.

```{r}
ma_1 <- stats::arima.sim(list(ma = 0.75), n = 1000)
ma_1_data <- RtsaPkg::get_series_data(series = ma_1)
ma_1_ts <- stats::ts(data = ma_1_data, start = c(1900,1), frequency = 12)
```

```{r}
#| fig-width: 8 
#| fig-cap: Simulated Moving Average Process ARIMA(0,0,1)

ma_1_dt <- RtsaPkg::tsObj_to_dt(series = ma_1_ts)
ma_1_ma <- RtsaPkg::graph_acf(
  df = ma_1_dt,
  time_col = "time",
  value_col = "value",
  max_lag = 30,
  confid_level = 1.96,
  ac_x_limits = c(1,30),
  ac_x_major_breaks = seq(1,30,1),
  ac_y_limits = c(-0.2, 0.6),
  pac_y_limits = c(-0.4, 0.6),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  line_color = "brown"
)
```

Show the impact of the moving average by simulating and plotting a ARIMA(0,0,2) process.

```{r}
ma_2 <- arima.sim(list(ma = c(0.65, 0.3)), n = 1000)
ma_2_data <- RtsaPkg::get_series_data(series = ma_2)
ma_2_ts <- stats::ts(data = ma_2_data, start = c(1900,1), frequency = 12)
```

```{r}
#| fig-width: 8 
#| fig-cap: Simulated Moving Average Process ARIMA(0,0,2)

ma_2_dt <- RtsaPkg::tsObj_to_dt(series = ma_2_ts)
ma_2_ma <- RtsaPkg::graph_acf(
  df = ma_2_dt,
  time_col = "time",
  value_col = "value",
  max_lag = 30,
  confid_level = 1.96,
  ac_x_limits = c(1,30),
  ac_x_major_breaks = seq(1,30,1),
  ac_y_limits = c(-0.2, 0.6),
  pac_y_limits = c(-0.4, 0.6),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  line_color = "brown"
)
```

The first two lags appear to be significant.

## Choosing between AR and MA terms

We need to consider both the ACF and PACF. Using these plots we can differentiate two signatures:

-   **AR signature**: The PACF of the differenced time series displays a sharp cutoff or the value at lag 1 in the PACF is positive.
-   **MA signature**: The ACF of the differenced time series displays a sharp cutoff. Commonly associated with a negative autocorrelation at lag 1 in the ACF of the differenced series.

::: topic
Impact of AR and MA terms together
:::

Plot and compare the ACF/PACF of ARIMA(1,0,1), ARIMA(2,0,1), ARIMA(2,0,2).

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-cap: The ACF/PACF of ARIMA(1,0,1), ARIMA(2,0,1), ARIMA(2,0,2)

# ARIMA(1,0,1)
ar_ma_1 <- arima.sim(list(order = c(1,0,1), ar = 0.8, ma = 0.8), n = 1000)
ar_ma_1_data <- RtsaPkg::get_series_data(series = ar_ma_1)
ar_ma_1_ts <- stats::ts(data = ar_ma_1_data, start = c(1900,1), frequency = 12)

ar_ma_1_dt <- RtsaPkg::tsObj_to_dt(series = ar_ma_1_ts)
ar_ma_1_arma <- RtsaPkg::graph_acf(
  df = ar_ma_1_dt,
  time_col = "time",
  value_col = "value",
  max_lag = 8,
  confid_level = 1.96,
  show_obs = F,
  ac_x_major_breaks = seq(0,30,5),
  ac_y_limits = c(-0.2,1),
  pac_x_major_breaks = seq(0,30,5),
  pac_y_limits = c(-0.6,1.0),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  row_height = 6,
  col_width = 7,
  display_plot = FALSE,
  line_color = "brown"
)

# ARIMA(2,0,1)
ar_ma_2 <- arima.sim(list(order = c(2,0,1), ar = c(0.6,0.3), ma = 0.8), n = 1000)
ar_ma_2_data <- RtsaPkg::get_series_data(series = ar_ma_2)
ar_ma_2_ts <- stats::ts(data = ar_ma_2_data, start = c(1900,1), frequency = 12)
ar_ma_2_dt <- RtsaPkg::tsObj_to_dt(series = ar_ma_2_ts)
ar_ma_2_arma <- RtsaPkg::graph_acf(
  df = ar_ma_2_dt,
  time_col = "time",
  value_col = "value",
  max_lag = 8,
  confid_level = 1.96,
  show_obs = F,
  ac_x_major_breaks = seq(0,30,5),
  ac_y_limits = c(-0.2,1),
  pac_x_major_breaks = seq(0,30,5),
  pac_y_limits = c(-0.4,1.0),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  row_height = 6,
  col_width = 7,
  display_plot = FALSE,
  line_color = "brown"
)

# ARIMA(2,0,2)
ar_ma_3 <- arima.sim(list(order = c(2,0,2), ar = c(0.6,0.3), ma = c(0.6,0.3)), n = 1000)
ar_ma_3_data <- RtsaPkg::get_series_data(series = ar_ma_3)
ar_ma_3_ts <- stats::ts(data = ar_ma_3_data, start = c(1900,1), frequency = 12)
ar_ma_3_dt <- RtsaPkg::tsObj_to_dt(series = ar_ma_3_ts)
ar_ma_3_arma <- RtsaPkg::graph_acf(
  df = ar_ma_3_dt,
  time_col = "time",
  value_col = "value",
  max_lag = 8,
  confid_level = 1.96,
  show_obs = F,
  ac_x_major_breaks = seq(0,30,5),
  ac_y_limits = c(-0.2,1),
  pac_x_major_breaks = seq(0,30,5),
  pac_y_limits = c(-0.4,1.0),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  row_height = 6,
  col_width = 7,
  display_plot = FALSE,
  line_color = "brown"
)

layout <- list(
  plots = list(ar_ma_1_arma$plots,ar_ma_2_arma$plots,ar_ma_3_arma$plots),
  rows = c(1,1,1),
  cols = c(1,2,3)
)
multi_plot <- RplotterPkg::multi_panel_grid(
  layout = layout,
  col_widths = rep(8, 3),
  row_heights = 8
)
```

## The expanded seasonal ARIMA -- the SARIMA Model

To model seasonal trends, we need to expand the ARIMA model with the seasonal parameters $P$, $D$, and $Q$ which correspond to $p$, $d$, $q$ in the original model.

-   $P$: number of seasonal autoregressive (SAR) terms
-   $D$: degree of seasonal (differencing)
-   $Q$: number of seasonal moving average (SMA) terms

The additional parameters are included into the ARIMA model in the following way: $$\Phi_P(B^S)\phi_P(B)(1 - B)^d(1 - B^S)y_t = \Theta_Q(B^S)\theta_q(B)\epsilon_t$$ Here $\Phi_P$ and $\Theta_Q$ are the coefficients for the seasonal AR and MA components respectively. $S$ is the period at which the seasonal trend occurs. For $S$ = 12 there is a yearly trend; for $S$ = 3 there is a quarterly trend.

## The expanded ARIMA with exogenous variables -- the ARIMAX model

ARIMAX stands for autoregressive integrated moving average with exogenous variables. An exogenous variable is a covariate, $x_t$, that influences the observed time series values $y_t$. ARIMAX can be specified by considering these $r$ exogenous variables according to the coefficient vector $\beta \in \mathbb{R}^r$: $$\phi_p(B)(1 - B)^dy_t = \beta^Tx_t\theta_q(B)\epsilon_t$$ Here $x_t \in \mathbb{R}^r$ is the $t$-th vector of exogenous features.

## ARIMA model for non-stationary data

Will use the `astsa::gtemp` data set.

```{r}
RplotterPkg::create_table(
  x = RtsaPkg::get_series_info(series = astsa::gtemp)$stats,
  container_width_px = 600
)
```

```{r}
#| fig-width: 8
#| fig-cap: Global Yearly Mean Land-Ocean Temperature Deviations (deg C)

gtemp_dt <- RtsaPkg::tsObj_to_dt(series = astsa::gtemp)
RplotterPkg::create_scatter_plot(
  df = gtemp_dt,
  aes_x = "time",
  aes_y = "value",
  rot_y_tic_label = TRUE,
  y_title = "Temperture (deg C)",
  x_limits = c(as.Date("1880-01-01"), as.Date("2010-01-01")),
  x_major_breaks = seq(as.Date("1880-01-01"), as.Date("2010-01-01"), "10 year"),
  x_date_labels = "%Y",
  connect = TRUE
)
```

Make the data stationary.

With the values increasing over time it will be necessary to difference the data. To make the data stationary use $d = 1$, first order difference.

```{r}
diff_v <- c(diff(gtemp_dt$value),NA)
gtemp_dt[, diff := diff_v]
gtemp_dt <- na.omit(gtemp_dt, cols = "diff")
str(gtemp_dt)
```

```{r}
#| fig-width: 8
#| fig-cap: Global Yearly Mean Land-Ocean Temperature Deviatioins, first order difference

RplotterPkg::create_scatter_plot(
  df = gtemp_dt,
  aes_x = "time",
  aes_y = "diff",
  rot_y_tic_label = TRUE,
  y_title = "Temperture (deg C)",
  x_limits = c(as.Date("1880-01-01"), as.Date("2010-01-01")),
  x_major_breaks = seq(as.Date("1880-01-01"), as.Date("2010-01-01"), "10 year"),
  x_date_labels = "%Y",
  connect = TRUE
)
```

The trend is removed and because the frequency is 1 year there is no seasonal component. To identify $p$ and $q$ consider the ACF and pACF plots:

```{r}
#| fig-width: 8
#| fig-cap: Global Yearly Mean Land-Ocean Temperature Deviations, ACF

gtemp_diff_ar <- RtsaPkg::graph_acf(
  df = gtemp_dt,
  time_col = "time",
  value_col = "diff",
  max_lag = 30,
  ac_x_limits = c(1,30),
  ac_x_major_breaks = seq(1,30,1),
  ac_y_limits = c(-0.4,0.4),
  ac_y_major_breaks = seq(-0.4,0.4,0.1),
  pac_y_limits = c(-0.4, 0.4),
  pac_y_major_breaks = seq(-0.4,0.4,0.1),
  confid_level = 1.96,
  show_obs = FALSE,
  show_minor_grids = FALSE,
  bold_y = 0.0,
  line_color = "brown"
)
```

Since the first lag's autocorrelation is negative use a moving average model. Thus set $p = 0$ and $q = 1$. This leads to ARIMA(0,1,1) model. Since the data are subject to increasing values, include a drift term in the model to take this effect into account:

```{r}
order_non_seasonal <- c(0,1,1)
gtemp_arima <- stats::arima(astsa::gtemp, order = order_non_seasonal)
```

Calculate and view the predictions.

```{r}
gtemp_predict <- stats::predict(gtemp_arima, n.ahead = 30)
```
